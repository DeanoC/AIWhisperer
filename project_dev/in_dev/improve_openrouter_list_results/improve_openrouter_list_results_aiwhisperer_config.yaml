natural_language_goal: Enhance the --list-models command to include an optional --output-csv
  parameter to export detailed model information from the OpenRouter API to a CSV
  file.
overall_context: |
  The goal is to improve the `--list-models` command of the ai-whisperer tool. The existing command only lists model names. The enhancement should allow users to get detailed model information, potentially exported to a CSV file.
plan:
  - step_id: planning_command_enhancement
  description: Analyze the requirements and structure the code changes needed to add
    the --output-csv parameter and integrate detailed OpenRouter API calls.
  depends_on: []
  agent_spec:
    type: planning
    input_artifacts:
      - main.py
      - OpenRouterAPI.py
    output_artifacts:
      - docs/list_models_enhancement_plan.md
    instructions: |
      Analyze the provided requirements for enhancing the `--list-models` command.
      Specifically, plan the necessary modifications to `main.py` to handle the new `--output-csv` argument and to the OpenRouter API interaction to fetch comprehensive model details.
      Outline the data fields that should be included in the detailed model information and how they map to potential CSV columns.
      Document the high-level implementation steps, including where code needs to be added or modified in `main.py` and `OpenRouterAPI.py`.
    constraints: []
    validation_criteria:
      - docs/list_models_enhancement_plan.md exists.
      - docs/list_models_enhancement_enhancement_plan.md clearly outlines changes
      needed in main.py for the new CLI argument.
      - docs/list_models_enhancement_plan.md clearly outlines changes needed in OpenRouterAPI.py
      to fetch detailed models.
      - docs/list_models_enhancement_plan.md identifies the key model details to be
      retrieved (features, cost, context window, provider, etc.).
      - docs/list_models_enhancement_plan.md outlines how the data will be formatted
      for console output and CSV.
  - step_id: test_generate_cli_parsing_tests
  description: Generate tests for the command-line argument parsing logic in main.py,
    specifically covering the new --output-csv parameter.
  depends_on:
    - planning_command_enhancement
  agent_spec:
    type: test_generation
    input_artifacts:
      - main.py
      - docs/list_models_enhancement_plan.md
    output_artifacts:
      - tests/unit/test_cli_parsing.py
    instructions: |
      Generate unit tests for the command-line argument parsing in `main.py`.
      Focus on testing the parsing logic related to the `--list-models` command and the new optional `--output-csv` parameter.
      Include test cases for:
      - "Calling `--list-models` without `--output-csv`."
      - "Calling `--list-models` with `--output-csv` pointing to a file path."
      - "Calling other commands to ensure `--output-csv` is only processed with `--list-models`."
      - "Invalid scenarios (e.g., `--output-csv` without `--list-models`)."
      The tests should use standard Python testing frameworks (like `unittest` or `pytest`) and mock necessary components if `main.py`'s parsing is tightly coupled.
      Ensure the tests assert the correct parsing of arguments and options.
    constraints:
      - Do not implement the actual CLI parsing logic in this step.
      - Focus solely on generating the test code.
    validation_criteria:
      - tests/unit/test_cli_parsing.py exists.
      - "tests/unit/test_cli_parsing.py contains tests specifically for `--list-models`
      with and without `--output-csv`."
      - "Tests cover different scenarios for the `--output-csv` argument."
  - step_id: implement_cli_parsing
  description: Modify main.py to add the --output-csv parameter parsing logic for
    the --list-models command.
  depends_on:
    - test_generate_cli_parsing_tests
  agent_spec:
    type: file_edit
    input_artifacts:
      - main.py
      - tests/unit/test_cli_parsing.py
      - docs/list_models_enhancement_plan.md
    output_artifacts:
      - main.py
    instructions: |
      Modify the `main.py` file to add an optional `--output-csv` argument specifically for the `--list-models` command.
      The argument should accept a file path where the CSV output will be saved.
      Ensure that the argument is only processed when `--list-models` is also specified.
      Store the parsed output file path in a variable accessible to the subsequent logic.
      Refer to the test cases in `tests/unit/test_cli_parsing.py` to guide the implementation, but do not implement code that only passes these specific tests. The code must correctly parse the arguments as per the requirements.
      Reuse existing argument parsing patterns in `main.py` if possible.
    constraints: []
    validation_criteria:
      - main.py is modified to include --output-csv argument parsing.
      - The argument parsing logic correctly handles --output-csv only when --list-models
      is present.
      - The parsed file path is stored.
  - step_id: validate_cli_parsing
  description: Run the generated tests to ensure the command-line argument parsing
    for --list-models and --output-csv works correctly.
  depends_on:
    - implement_cli_parsing
  agent_spec:
    type: validation
    input_artifacts:
      - main.py
      - tests/unit/test_cli_parsing.py
    output_artifacts: []
    instructions: |
      Execute the tests located in `tests/unit/test_cli_parsing.py`.
      Use `pytest` to run the tests.
      Report the test results.
    constraints: []
    validation_criteria:
      - "The command `pytest tests/unit/test_cli_parsing.py` executes successfully."
      - All tests in tests/unit/test_cli_parsing.py pass.
  - step_id: test_generate_api_fetch_tests
  description: Generate tests for the OpenRouterAPI.list_models method to ensure it
    fetches comprehensive model details.
  depends_on:
    - planning_command_enhancement
  agent_spec:
    type: test_generation
    input_artifacts:
      - OpenRouterAPI.py
      - docs/list_models_enhancement_plan.md
    output_artifacts:
      - tests/unit/test_openrouter_api.py
    instructions: |
      Generate unit tests for the `OpenRouterAPI.list_models()` method.
      These tests should verify that the method correctly fetches comprehensive model details, including features, cost, context window size, provider information, and other relevant attributes, rather than just a list of names.
      Use mocking to simulate the OpenRouter API response, providing a mock response structure that includes the expected detailed fields.
      Test cases should assert that the `list_models` method returns a structured list of objects or dictionaries, each containing the expected detailed fields for a model.
    constraints:
      - Do not modify the OpenRouterAPI.py file in this step.
      - Focus solely on generating the test code and mock responses.
    validation_criteria:
      - tests/unit/test_openrouter_api.py exists.
      - "tests/unit/test_openrouter_api.py contains tests for `OpenRouterAPI.list_models`."
      - Tests mock the OpenRouter API response to simulate detailed model data.
      - "Tests assert the structure and content of the data returned by `list_models`."
  - step_id: enhance_api_fetching
  description: Modify OpenRouterAPI.py to enhance the list_models method to retrieve
    comprehensive model details.
  depends_on:
    - test_generate_api_fetch_tests
  agent_spec:
    type: file_edit
    input_artifacts:
      - OpenRouterAPI.py
      - tests/unit/test_openrouter_api.py
      - docs/list_models_enhancement_plan.md
    output_artifacts:
      - OpenRouterAPI.py
    instructions: |
      Modify the `OpenRouterAPI.list_models()` method in `OpenRouterAPI.py`.
      Change the API call or processing logic within this method to retrieve comprehensive details for each model available from OpenRouter, as identified in the planning document and the test cases in `tests/unit/test_openrouter_api.py`.
      Ensure the method returns a list of structured objects or dictionaries, where each item represents a model and contains fields for features/capabilities, cost (input/output tokens), context window size, provider information, and any other relevant attributes available from the API.
      The goal is to fetch and return the detailed metadata, not just the model names.
      Reuse existing API communication patterns within `OpenRouterAPI.py`.
    constraints: []
    validation_criteria:
      - OpenRouterAPI.py is modified.
      - "The `list_models` method now fetches and returns detailed model information
      structured as objects or dictionaries."
      - The returned data structure includes fields for cost, context window, and
      provider, at a minimum.
  - step_id: validate_api_fetching
  description: Run the generated tests to ensure the OpenRouterAPI.list_models method
    fetches detailed information correctly.
  depends_on:
    - enhance_api_fetching
  agent_spec:
    type: validation
    input_artifacts:
      - OpenRouterAPI.py
      - tests/unit/test_openrouter_api.py
    output_artifacts: []
    instructions: |
      Execute the tests located in `tests/unit/test_openrouter_api.py`.
      Use `pytest` to run the tests.
      Report the test results.
    constraints: []
    validation_criteria:
      - "The command `pytest tests/unit/test_openrouter_api.py` executes successfully."
      - All tests in tests/unit/test_openrouter_api.py pass.
  - step_id: test_generate_output_formatting_tests
  description: Generate tests for formatting the detailed model data for console and
    CSV output.
  depends_on:
    - validate_api_fetching
  agent_spec:
    type: test_generation
    input_artifacts:
      - main.py
      - docs/list_models_enhancement_plan.md
    output_artifacts:
      - tests/unit/test_output_formatting.py
    instructions: |
      Generate unit tests for the logic responsible for formatting the detailed model data for both console output and CSV export.
      Create mock detailed model data (structured as returned by the enhanced `OpenRouterAPI.list_models` method).
      Test cases should cover:
      - Formatting the data for console display (e.g., ensuring readable output with key details).
      - Formatting the data for CSV export (e.g., ensuring correct header row and properly quoted/escaped data rows).
      - Handling cases with missing data fields.
      The tests should assert the structure and content of the formatted output strings or data structures.
    constraints:
      - Do not implement the output formatting logic in this step.
      - Focus solely on generating the test code and mock data.
    validation_criteria:
      - tests/unit/test_output_formatting.py exists.
      - tests/unit/test_output_formatting.py contains tests for both console and CSV
      formatting.
      - Tests use mock detailed model data.
      - Tests assert the correctness of formatted output.
  - step_id: implement_output_formatting
  description: Add logic to main.py to format the detailed model data for console
    and CSV output based on the --output-csv argument.
  depends_on:
    - test_generate_output_formatting_tests
    - validate_cli_parsing
  agent_spec:
    type: file_edit
    input_artifacts:
      - main.py
      - tests/unit/test_output_formatting.py
      - docs/list_models_enhancement_plan.md
    output_artifacts:
      - main.py
    instructions: |
      Modify the `main.py` file within the section handling the `--list-models` command.
      After fetching the detailed model data using the enhanced `OpenRouterAPI.list_models` method, add logic to process this data.
      - "If the `--output-csv` argument was provided during parsing, format the detailed data into a CSV string or structure (including a header row) and write it to the specified file path. Use a standard CSV library if available."
      - "If the `--output-csv` argument was NOT provided, format the detailed data for clear and human-readable output to the console (stdout)."
      Ensure the formatting includes the key details identified in the planning step and tested in `tests/unit/test_output_formatting.py`.
      Handle potential errors during file writing.
      Reuse existing printing or file writing patterns in `main.py`.
    constraints: []
    validation_criteria:
      - main.py is modified to include output formatting logic.
      - The logic correctly differentiates between console and CSV output based on
      the --output-csv argument.
      - Console output is formatted clearly.
      - CSV output includes a header and data rows with relevant model details.
  - step_id: validate_output_formatting
  description: Run the generated tests to ensure the model data is formatted correctly
    for both console and CSV.
  depends_on:
    - implement_output_formatting
  agent_spec:
    type: validation
    input_artifacts:
      - main.py
      - tests/unit/test_output_formatting.py
    output_artifacts: []
    instructions: |
      Execute the tests located in `tests/unit/test_output_formatting.py`.
      Use `pytest` to run the tests.
      Report the test results.
    constraints: []
    validation_criteria:
      - "The command `pytest tests/unit/test_output_formatting.py` executes successfully."
      - All tests in tests/unit/test_output_formatting.py pass.
  - step_id: test_generate_e2e_tests
  description: Generate end-to-end tests for the enhanced --list-models command, covering
    both console and CSV output scenarios.
  depends_on:
    - validate_output_formatting
  agent_spec:
    type: test_generation
    input_artifacts:
      - main.py
      - OpenRouterAPI.py
      - docs/list_models_enhancement_plan.md
    output_artifacts:
      - tests/integration/test_list_models_e2e.py
    instructions: |
      Generate end-to-end tests for the enhanced `ai-whisperer --list-models` command.
      These tests should simulate running the command with and without the `--output-csv` argument.
      Test cases should cover:
      - "Running `ai-whisperer --list-models` and verifying that detailed model information is printed to standard output in a readable format. Use mocking for the OpenRouter API call to ensure predictable test data."
      - "Running `ai-whisperer --list-models --output-csv models_output.csv`, checking that a file named `models_output.csv` is created, and verifying its content (header row and data rows with expected details and structure). Use mocking for the OpenRouter API call."
      - "Clean up any generated files (like `models_output.csv`) after the test runs."
      Use appropriate mocks for the OpenRouter API call during these end-to-end tests.
    constraints:
      - Do not modify the application code in this step.
      - Focus solely on generating the test code.
    validation_criteria:
      - tests/integration/test_list_models_e2e.py exists.
      - tests/integration/test_list_models_e2e.py contains test cases for both console
      and CSV output scenarios.
      - Tests use mocks for the OpenRouter API.
      - Tests include cleanup for generated files.
  - step_id: validate_e2e_functionality
  description: Run the end-to-end tests to confirm the enhanced --list-models command
    works as expected in both console and CSV modes.
  depends_on:
    - test_generate_e2e_tests
    - implement_output_formatting
  agent_spec:
    type: validation
    input_artifacts:
      - main.py
      - OpenRouterAPI.py
      - tests/integration/test_list_models_e2e.py
    output_artifacts: []
    instructions: |
      Execute the end-to-end tests located in `tests/integration/test_list_models_e2e.py`.
      Use `pytest` to run the tests.
      Ensure necessary test dependencies (like mocking libraries) are available.
      Report the test results.
    constraints: []
    validation_criteria:
      - "The command `pytest tests/integration/test_list_models_e2e.py` executes successfully."
      - All tests in tests/integration/test_list_models_e2e.py pass.
  - step_id: update_documentation
  description: Update documentation (README and CLI help) to reflect the new --output-csv
    parameter for the --list-models command.
  depends_on:
    - validate_e2e_functionality
  agent_spec:
    type: documentation
    input_artifacts:
      - README.md
      - main.py   # To extract help text
    output_artifacts:
      - README.md
    instructions: |
      Update the project's documentation to include details about the new `--output-csv` optional parameter for the `--list-models` command.
      Specifically:
      1. Modify `README.md` to add a section or update the existing `--list-models` description to explain the `--output-csv` option, its purpose, and provide an example usage similar to the one in the requirements.
      2. Ensure the command-line help text generated by the application (which is often derived from the argument parsing setup in `main.py`) is updated to accurately describe the `--output-csv` parameter. This might involve adding help text strings in the `main.py` code where the argument is defined.
      The documentation should clearly explain how users can export detailed model information to a CSV file.
    constraints: []
    validation_criteria:
      - README.md is updated to mention the --output-csv parameter.
      - README.md includes an example of using --output-csv.
      - The CLI help message for --list-models includes documentation for --output-csv.
