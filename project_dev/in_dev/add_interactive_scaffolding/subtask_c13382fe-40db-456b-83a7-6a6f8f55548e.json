{
  "description": "Generate tests for the interactive list-models prompt.",
  "instructions": [
    "Create a new Python file `tests/integration/test_list_models_interactive.py`.",
    "Import necessary modules for testing, mocking, and interacting with the CLI.",
    "Define a test class, e.g., `TestListModelsInteractive`.",
    "Within the test class, create test methods for each verification point:",
    "- A test method to verify that after running the list-models command, an interactive prompt appears. This might involve capturing the output stream and checking for specific prompt text or patterns.",
    "- A test method to verify that user input provided to the interactive delegate is correctly captured and processed. This will require mocking the delegate's input method.",
    "- A test method to verify that a mock AI response sent via the delegate is displayed in the UI. This will require mocking the delegate's output display method and checking the captured output stream.",
    "- A test method to verify that the delegate correctly routes user input to the AI interaction part of the application. This will require mocking the delegate and the AI interaction component to ensure the correct methods are called with the expected input.",
    "Use mocking frameworks (like `unittest.mock`) to isolate the tests from actual external dependencies (AI APIs, user input).",
    "Ensure the tests are structured to compile and run without errors.",
    "The tests should be designed to *fail* when run against the current implementation, as the interactive feature is not yet fully built. This confirms the tests are correctly targeting the unimplemented functionality."
  ],
  "input_artifacts": [
    "docs/list_models_interaction_analysis.md"
  ],
  "output_artifacts": [
    "tests/integration/test_list_models_interactive.py"
  ],
  "constraints": [
    "Tests must heavily rely on mocking to simulate AI responses and delegate interactions.",
    "No actual external API calls (e.g., to AI services) should be made during test execution.",
    "The tests should target the interactive flow triggered by the list-models command.",
    "The tests should be placed within the `tests/integration` directory."
  ],
  "validation_criteria": [
    "The file `tests/integration/test_list_models_interactive.py` is created.",
    "The file `tests/integration/test_list_models_interactive.py` contains test functions or methods.",
    "The tests cover the scenarios of prompt appearance, user input capture, AI response display, and delegate routing.",
    "Running `pytest tests/integration/test_list_models_interactive.py` executes the tests.",
    "The tests are designed to initially fail, indicating they correctly target the unimplemented interactive feature."
  ],
  "type": "test_generation",
  "name": "test_list_models_interaction_generation",
  "depends_on": [
    "analyze_list_models_interaction"
  ],
  "task_id": "32fb08e9-d83b-4027-bdc5-ed57c1604243",
  "subtask_id": "c13382fe-40db-456b-83a7-6a6f8f55548e"
}