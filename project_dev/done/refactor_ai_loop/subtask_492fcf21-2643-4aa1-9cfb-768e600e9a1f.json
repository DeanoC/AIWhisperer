{
  "description": "Run existing integration tests to ensure the refactor hasn't broken anything.",
  "instructions": [
    "Navigate to the root directory of the AIWhisperer project.",
    "Ensure the Python environment is activated and dependencies are installed (e.g., `pip install -r requirements.txt`).",
    "Execute the integration test script using pytest: `pytest tests/integration/test_run_plan_script.py`.",
    "Monitor the output of the test execution.",
    "Identify any test failures or errors reported by pytest.",
    "If any tests fail, analyze the output to understand the cause of the failure.",
    "If all tests pass, the refactor is validated against this specific integration test."
  ],
  "input_artifacts": [
    "src/ai_whisperer/",
    "tests/integration/test_run_plan_script.py",
    "requirements.txt"
  ],
  "output_artifacts": [],
  "constraints": [
    "The tests must be run in an environment where all project dependencies are installed.",
    "No modifications should be made to the test script itself.",
    "The test execution must complete without critical errors that prevent tests from running."
  ],
  "validation_criteria": [
    "The command `pytest tests/integration/test_run_plan_script.py` completes successfully.",
    "The pytest output indicates that all tests within `test_run_plan_script.py` passed.",
    "No new test failures are reported compared to the test results before the refactor."
  ],
  "type": "validation",
  "name": "validate_refactor_with_existing_tests",
  "depends_on": [
    "integrate_context_manager_and_ai_loop"
  ],
  "task_id": "1bc14942-c2cc-4e02-aa1a-78cfb861d8e9",
  "subtask_id": "492fcf21-2643-4aa1-9cfb-768e600e9a1f"
}